{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Teacher Networks\n",
    "\n",
    "Training thin deep networks following the student-teacher learning paradigm has received intensive attention because of its excellent performance. In such a paradigm, there is a huge neural network known as the teacher network which is expert at performing a certain task. There is also a much smaller student network which learns to perform the same task using some form of guidance from the teacher. \n",
    "\n",
    "The student can be small in terms of 1) Depth 2) Number of parameters.\n",
    "\n",
    "The guidance is provided by the teacher network based on hints in some form or the other. In this notebook we will see one such setup where the guidance is provided by the outputs of the teacher network.\n",
    "\n",
    "Here are the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='../../data/lab6',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../../data/lab6',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Teacher Network\n",
    "\n",
    "A comparitively bigger and deeper network as compared to the student network defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Teacher, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc1 = nn.Linear(7*7*32, 300)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the student network\n",
    "\n",
    "A comparitively smaller and shallower network than the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Student, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc1 = nn.Linear(14*14*16, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_model(is_teacher = True):\n",
    "    if is_teacher == True:\n",
    "        net = Teacher()\n",
    "    else:\n",
    "        net = Student()\n",
    "    net = net.cuda()\n",
    "\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    return net,criterion,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the teacher network\n",
    "\n",
    "The first step is to train the teacher network to become an expert. We move ahead with regular training procedure using the cross entropy loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher, criterion, optimizer = reset_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "def training(net, reset = True):\n",
    "    if reset == True:\n",
    "        net, criterion, optimizer = reset_model()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        accuracy = []\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            temp_labels = labels\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data[0]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == temp_labels).sum()\n",
    "            accuracy.append(correct/float(batch_size))\n",
    "\n",
    "        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the teacher network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "def testing(net):\n",
    "    net.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        images = Variable(images)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 77.1952, Accuracy: 0.9619\n",
      "Epoch: 2, Loss: 30.7803, Accuracy: 0.9840\n",
      "Epoch: 3, Loss: 22.5943, Accuracy: 0.9882\n",
      "Epoch: 4, Loss: 20.9438, Accuracy: 0.9887\n",
      "Epoch: 5, Loss: 16.7684, Accuracy: 0.9909\n",
      "Test Accuracy of the network on the 10000 test images: 99.03 %\n"
     ]
    }
   ],
   "source": [
    "reset = True\n",
    "teacher = training(teacher, reset)\n",
    "testing(teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for Student Network\n",
    "\n",
    "Here, we define a few more parameters of the student network. In the student network, we will train with the soft targets as well the hard targets. The soft targets will be calculated by the following equation:\n",
    "\n",
    "$$\n",
    "f(z_{i}) = \\frac{\\exp(z_{i})}{\\sum_{j}\\exp(z_{j})}\n",
    "$$\n",
    "\n",
    "This results in softening out the outputs of the teacher and this can be used as hints for the student network.\n",
    "<img src='images/stud_teach.png', style=\"width: 350px;\">\n",
    "\n",
    "The loss doesn't need to get backpropagated accross the teacher network and therefore we make the corresponding modification.\n",
    "\n",
    "Also, for training witht he soft labels, we use mean square error loss since using a Cross Entropy loss for soft labels makes no sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student (\n",
      "  (layer1): Sequential (\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (fc1): Linear (3136 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.5\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad= False\n",
    "\n",
    "student, criterion, optimizer = reset_model(is_teacher = False)\n",
    "alpha = 0.6\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "print student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the student network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 354.3688, Accuracy: 0.9353\n",
      "Epoch: 2, Loss: 313.1935, Accuracy: 0.9691\n",
      "Epoch: 3, Loss: 305.5597, Accuracy: 0.9732\n",
      "Epoch: 4, Loss: 300.7783, Accuracy: 0.9759\n",
      "Epoch: 5, Loss: 297.6083, Accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    accuracy = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        temp_labels = labels\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        student_outputs = student(images)\n",
    "        \n",
    "        hard_outputs = teacher(images)\n",
    "        soft_outputs = hard_outputs/ temperature\n",
    "        soft_outputs = softmax(soft_outputs)\n",
    "        \n",
    "        hard_loss = criterion(student_outputs, labels)\n",
    "        soft_loss = mse_criterion(student_outputs, soft_outputs)\n",
    "        loss = alpha*hard_loss + (1-alpha)*soft_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data[0]\n",
    "        _, predicted = torch.max(student_outputs.data, 1)\n",
    "        correct = (predicted == temp_labels).sum()\n",
    "        accuracy.append(correct/float(batch_size))\n",
    "    \n",
    "    print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the network on the 10000 test images: 97.69 %\n"
     ]
    }
   ],
   "source": [
    "testing(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Excercise\n",
    "\n",
    "Try out the small student network on the CIFAR dataset. (Easy enough to load with the data loader!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. https://arxiv.org/abs/1412.6550\n",
    "2. https://www.cs.toronto.edu/~hinton/absps/distillation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
